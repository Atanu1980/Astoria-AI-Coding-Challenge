{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16de7336",
      "metadata": {
        "id": "16de7336"
      },
      "source": [
        "# Langchain based Interview Agent\n",
        "\n",
        "This notebook demonstrates the development of an AI Interview Agent that can dynamically generate interview questions based on a provided job posting and candidate resume. Leveraging LangChain, we enable more advanced techniques for document parsing, output processing, and LLM interaction, allowing for the creation of comprehensive question-generation pipelines.\n",
        "\n",
        "LangChain's framework offers flexibility by removing the requirement to use specific SDKs, empowering us to select and integrate any LLM model along with its functions and tools."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Information extraction and parsing\n",
        "\n",
        "This section covers parsing and extracting essential information from the provided documents (such as the job post and candidate resume) to build the AI Interview Agent."
      ],
      "metadata": {
        "id": "dQBhrytd59dl"
      },
      "id": "dQBhrytd59dl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1. Loding model environment and API keys**\n",
        "\n",
        "For this project, we utilize OpenAI's **GPT-4** model to handle information extraction and question generation. Here, we initialize the model environment and securely load the necessary API keys for seamless interaction with the model throughout the process.\n"
      ],
      "metadata": {
        "id": "aPec7wU-0Ddh"
      },
      "id": "aPec7wU-0Ddh"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cb41f5f4-df8d-4d04-9eaa-193b8c29b00b",
      "metadata": {
        "height": 115,
        "tags": [],
        "id": "cb41f5f4-df8d-4d04-9eaa-193b8c29b00b"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2. Loading libraries for document parsing**\n",
        "\n",
        "In this project, I have used the **PyMuPDF** library for parsing PDF documents.\n",
        "I chose PyMuPDF for its high accuracy in extracting text while preserving the original formatting, making it well-suited for processing job posts and resumes, which are widely available in PDF format."
      ],
      "metadata": {
        "id": "VouTM4GH88st"
      },
      "id": "VouTM4GH88st"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymupdf python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEqbGjLdICbC",
        "outputId": "71db1652-598c-488e-b1e9-ed7f6291fad5"
      },
      "id": "xEqbGjLdICbC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import docx\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "aN3ntV4g5q_e"
      },
      "id": "aN3ntV4g5q_e",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_file(file_path: str) -> str:\n",
        "  file_path = Path(file_path)\n",
        "\n",
        "  if file_path.suffix.lower() == '.pdf':\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\")  # 'text' mode preserves the original spacing\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "  elif file_path.suffix.lower() == '.docx':\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    return text\n",
        "\n",
        "  elif file_path.suffix.lower() == '.txt':\n",
        "    return file_path.read_text()\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f\"Unsupported file type: {file_path.suffix}\")"
      ],
      "metadata": {
        "id": "OqDKkXbP5Qew"
      },
      "id": "OqDKkXbP5Qew",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3. Using functions for information extraction**\n",
        "\n",
        "In the cells below, we define three specialized functions to extract relevant information from:\n",
        "- The candidate's resume\n",
        "- The job description\n",
        "- The company profile\n",
        "\n",
        "For easier function definition and organization, we use **Pydantic** classes, which allow us to describe and construct functions without needing to manually define complex JSON structures.\n",
        "The utility function `convert_pydantic_to_openai_function` automatically\n",
        "converts our Pydantic classes into OpenAI-compatible function formats, streamlining the process of passing structured data to and from the model.\n"
      ],
      "metadata": {
        "id": "ylErMYhF-FAk"
      },
      "id": "ylErMYhF-FAk"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bfb2ede9-b0b4-4c8c-b00f-9a9911f38614",
      "metadata": {
        "height": 64,
        "id": "bfb2ede9-b0b4-4c8c-b00f-9a9911f38614"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b3ae5481-4155-4831-a5dd-4c183b3b4990",
      "metadata": {
        "height": 98,
        "id": "b3ae5481-4155-4831-a5dd-4c183b3b4990"
      },
      "outputs": [],
      "source": [
        "class ResumeParser(BaseModel):\n",
        "    \"\"\"Extract the following information from the resume\"\"\"\n",
        "    Name: str = Field(description=\"candidate's name\")\n",
        "    Education: str = Field(description=\"educational detail's such as degree, major/field of study, institution name and graduation date.\")\n",
        "    Work_experience: str = Field(description=\"Total years of experience and details including job title, company, dates, key responsibilities and achievements. Also summarize major projects, accomplishments, or publications\")\n",
        "    Skills: str = Field(description=\"list technical skills, programming languages, frameworks, tools, and other relevant skills\")\n",
        "    Certifications: str = Field(description=\"list certifications with the name, issuing organization, and issue date.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class JobPostingParser(BaseModel):\n",
        "    \"\"\"Extract the following information from the job posting\"\"\"\n",
        "    Job_title: str = Field(description = \"The job title for this position.\")\n",
        "    Responsibilities: str = Field(description=\"The main duties and responsibilities.\")\n",
        "    Required_skills : str = Field(description=\"The skills and qualifications explicitly required for this role.\")\n",
        "    Preferred_skills : str = Field(\"The skills and qualifications that are preferred but not mandatory.\")\n",
        "    Required_experience: str = Field(description=\"The level of experience required, including years and specific areas.\")\n",
        "    Location: str = Field(description=\"The location, including remote options if available.\")"
      ],
      "metadata": {
        "id": "8liCUUciqHtc"
      },
      "id": "8liCUUciqHtc",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CompanyProfile(BaseModel):\n",
        "    \"\"\"Extract the following information from the company profile\"\"\"\n",
        "    Company_Name: str = Field(description=\"The name of the company.\")\n",
        "    Industry: str = Field(description=\"The industry or industries this company operates within.\")\n",
        "    Mission: str = Field(description=\"The mission statement that explains the company's purpose and objectives.\")\n",
        "    Vision: str = Field(description=\"The vision statement that describes the company’s long-term goals.\")\n",
        "    Core_values: str = Field(\"The key principles or beliefs that define the company’s identity.\")\n",
        "    Company_culture: str = Field(\" A description of the work environment, team dynamics, and company atmosphere\")"
      ],
      "metadata": {
        "id": "Uhoejvtz-vJU"
      },
      "id": "Uhoejvtz-vJU",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = extract_text_from_file('/content/Atanu_Dahari_Resume.pdf')\n",
        "job_posting_text = extract_text_from_file('/content/Senior AI Research Engineer.docx')\n",
        "company_profile_text = extract_text_from_file('/content/NRG company values.txt')"
      ],
      "metadata": {
        "id": "1gRGIQ2qOEOs"
      },
      "id": "1gRGIQ2qOEOs",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-HyQY5QrxP1",
        "outputId": "7ae48dfb-767a-49ca-be7c-d597cab7eb56"
      },
      "id": "U-HyQY5QrxP1",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ed8b15a7-450c-43d6-af44-ca63800a4619",
      "metadata": {
        "height": 47,
        "id": "ed8b15a7-450c-43d6-af44-ca63800a4619"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2cc5bac3-3783-4ce7-9de6-83c905fba7c5",
      "metadata": {
        "height": 30,
        "id": "2cc5bac3-3783-4ce7-9de6-83c905fba7c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c522d396-0bea-421a-f607-2d57347ce739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c840131923b3>:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  model = ChatOpenAI(\n"
          ]
        }
      ],
      "source": [
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=OPENAI_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "798ac5b3-7e47-4cfb-8173-63900cf1e7f6",
      "metadata": {
        "height": 30,
        "id": "798ac5b3-7e47-4cfb-8173-63900cf1e7f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b319d8-c80c-4ebf-befb-7e981432b4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-f5ae804d5cab>:1: LangChainDeprecationWarning: The function `convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
            "  extracting_functions = [convert_pydantic_to_openai_function(ResumeParser), convert_pydantic_to_openai_function(JobPostingParser), convert_pydantic_to_openai_function(CompanyProfile) ]\n"
          ]
        }
      ],
      "source": [
        "extracting_functions = [convert_pydantic_to_openai_function(ResumeParser), convert_pydantic_to_openai_function(JobPostingParser), convert_pydantic_to_openai_function(CompanyProfile) ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract the relevant information, based on the function requested. If not explicitly provided do not guess.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "MZnMcGKGSVxv"
      },
      "id": "MZnMcGKGSVxv",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
      ],
      "metadata": {
        "id": "wDp_DwCSzqxp"
      },
      "id": "wDp_DwCSzqxp",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_model = model.bind(functions=extracting_functions)"
      ],
      "metadata": {
        "id": "mwx6V30WCmC4"
      },
      "id": "mwx6V30WCmC4",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1d0bc519-2b8e-40d0-88ec-fc5ef0291f16",
      "metadata": {
        "height": 30,
        "id": "1d0bc519-2b8e-40d0-88ec-fc5ef0291f16"
      },
      "outputs": [],
      "source": [
        "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4. Extraction output**\n",
        "\n",
        "Below is the output from the extraction chains. Here, we utilize chains to seamlessly combine the prompt, model, and output parser into a single processing pipeline. This setup allows us to pass documents through the pipeline, where the model generates structured outputs that are automatically parsed and ready for further processing in the interview question generation workflow."
      ],
      "metadata": {
        "id": "8W2fzuZzCVnl"
      },
      "id": "8W2fzuZzCVnl"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8688fdba-0996-446c-a77b-318094944998",
      "metadata": {
        "height": 30,
        "id": "8688fdba-0996-446c-a77b-318094944998"
      },
      "outputs": [],
      "source": [
        "resume_info = extraction_chain.invoke({resume_text})\n",
        "job_posting_info = extraction_chain.invoke({job_posting_text})\n",
        "company_profile_info = extraction_chain.invoke({company_profile_text})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM4I-JulJ6GY",
        "outputId": "0801f8be-ed66-4004-f69d-cd2f851556a9"
      },
      "id": "LM4I-JulJ6GY",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Name': 'ATANU DAHARI',\n",
              " 'Education': 'Master of Computer Science (MCS) from Rice University, Houston, Texas (August 2021 – December 2022), CGPA: 3.43 / 4.0. Relevant coursework: Deep Learning, Computational Statistics, R, Database management Systems, Web development. Bachelor of Technology in Electronics and Communication Technology (B. Tech in ECE) from Vellore Institute of Technology, Vellore, India (July 2017 - June 2021), CGPA: 8.62 / 10.0. Relevant coursework: Data Structures and Algorithms, Cloud Computing, Object oriented programming, Advanced Java.',\n",
              " 'Work_experience': 'Machine Learning Engineer at Circle.ooo, Houston, TX (February 2024 – Present): Created an AI assistant using RAG for an event hosting platform, integrated data from various APIs and vector databases, automated event creation, resulting in a 25% increase in user interaction. AI Engineer at WarrantyMe, Houston, TX (May 2023 – January 2024): Developed a warranty information extraction system, extracted warranty data using transformer models and NER algorithms, developed an RPA agent to file warranty claims, increasing user convenience. Machine Learning Engineer at Affekta LLC, Houston, TX (June 2023 – December 2023): Deployed computer vision models to interpret real-time emotions of students, optimized deployment efficiency, reducing setup time by 30%. Project Experience: Green Connex AI Recycling Project - developed a computer vision AI model using PyTorch, achieved an accuracy of 81.1%. Detected harmful social media memes using a multi-modal model, achieving an accuracy of 57.8%. Graduate Research Assistant at Rice University - optimized search algorithms in IoT applications, achieved a 15x performance boost. Flight delay and experience analysis using R - conducted analysis using R libraries, developed interactive dashboards using Shiny.',\n",
              " 'Skills': 'Retrieval Augmented Generation (RAG), Computer Vision, Natural Language Processing (NLP), Robotic Process Automation (RPA), Single Instruction Multiple Data (SIMD). Databases: PostgreSQL, SQLite, MongoDB, Neo4j. Languages: C/C++, Java, Python, MATLAB, HTML5, CSS, JavaScript, React, Node.js. Tools: IntelliJ, Visual Studio, R Studio, Azure Machine Learning Studio, Google Cloud Platform, Microsoft Power Automate.',\n",
              " 'Certifications': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_posting_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mMufFqZUkWJ",
        "outputId": "0303ddcf-d111-47f0-b49f-28f70ce2c375"
      },
      "id": "9mMufFqZUkWJ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Job_title': 'Senior AI Research Engineer',\n",
              " 'Responsibilities': 'Advanced Model Architecture: Develop and optimize transformer-based models for NLP tasks, ensuring scalability and efficiency in the cloud environment (Azure/AWS). Generative AI Development: Build and refine generative models, particularly LLMs, to generate coherent and contextually appropriate text for applications like customer care and chatbot automation. AI Integration: Work on integrating Azure OpenAI, Assistant API, and other LLM solutions into practical AI-driven products, such as chatbots and virtual assistants. Research and Innovation: Lead AI research initiatives focusing on cutting-edge transformer and NLP methodologies, exploring new applications and improving model performance. Technical Leadership: Provide leadership in PyTorch, transformers, and LLMs, guiding the development team through complex training and deployment challenges. Collaboration & Mentorship: Collaborate with cross-functional teams to incorporate AI technologies into diverse products and mentor junior engineers and researchers.',\n",
              " 'Required_skills': 'Deep expertise in NLP and generative AI, particularly with transformer-based architectures. Proficiency in PyTorch, understanding its mechanisms and optimization techniques. Hands-on knowledge of Azure OpenAI and Assistant APIs for developing AI-powered solutions. Advanced techniques in model fine-tuning, regularization, and LLM interpretability. Programming expertise in Python and at least one additional language.',\n",
              " 'Preferred_skills': 'Published research or contributions to open-source AI projects. Experience in cloud-based AI model deployment on platforms like Azure, AWS, or Google Cloud. Familiarity with AI-driven automation solutions (e.g., chatbots, customer care automation).',\n",
              " 'Required_experience': 'Proven hands-on experience in transformer architectures, LLMs, and NLP. Demonstrated success in building and deploying large-scale generative AI models using PyTorch. LLM integration experience with Google and OpenAI. Familiarity with Assistant APIs and Chat Completions technologies. Experience in multi-cloud environments, including Azure and AWS.',\n",
              " 'Location': 'Not specified'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "company_profile_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrllY0oHUo9Y",
        "outputId": "d978c275-aa9b-492f-b8e3-23c173fd5dfa"
      },
      "id": "UrllY0oHUo9Y",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Company_Name': 'NRG',\n",
              " 'Industry': 'Energy and Home Services',\n",
              " 'Mission': 'Driven by the idea of a smarter, cleaner future, focusing on innovative solutions that make customers’ lives easier by helping them power, protect, and intelligently manage their homes and businesses.',\n",
              " 'Vision': 'Creating possibilities to empower the millions of customers we serve and communities where they live and work.'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e417b4-9306-413f-865f-e13dbd2d0196",
      "metadata": {
        "id": "92e417b4-9306-413f-865f-e13dbd2d0196"
      },
      "source": [
        "## 2. Interview questions script generation  \n",
        "\n",
        "This section focuses on generating a personalized interview questions script\n",
        "using the information extracted from the documents. The prompts are carefully designed to create a balanced mix of technical and behavioral questions,\n",
        "ensuring a comprehensive interview that assesses both skill fit and cultural alignment with the company.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "97338dce-a61a-4f8b-912c-6108dcb86183",
      "metadata": {
        "height": 98,
        "id": "97338dce-a61a-4f8b-912c-6108dcb86183"
      },
      "outputs": [],
      "source": [
        "technical_questions_prompt = \"\"\"Generate {num_questions} technical interview questions based on the job requirements and Candidate's resume:\n",
        "\n",
        "        Requirements:\n",
        "        1. Questions should test both job requirements and candidate skills.\n",
        "        2. Include a mix of difficulty levels.\n",
        "        3. Make them specific to the candidate's experience level.\"\"\"\n",
        "\n",
        "behavioral_questions_prompt = \"\"\"Generate {num_questions} behavioral interview questions based on the candiadte's resume and the company profile:\n",
        "\n",
        "        Requirements:\n",
        "        1. Questions should assess cultural fit for the role.\n",
        "        2. Questions should reveal candidate's alignment with company values.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f558fb45-f3a5-47be-8778-dddec2d00ba1",
      "metadata": {
        "height": 64,
        "id": "f558fb45-f3a5-47be-8778-dddec2d00ba1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "technical_num_questions = 5\n",
        "\n",
        "# Format the technical questions prompt content separately\n",
        "technical_system_content = technical_questions_prompt.format(num_questions = technical_num_questions)\n",
        "\n",
        "# Define the user message content using f-string formatting\n",
        "technical_user_content = f\"\"\"\n",
        "Candidate's resume:\n",
        "Name: {resume_info.get('Name', 'N/A')}\n",
        "Education: {resume_info.get('Education', 'N/A')}\n",
        "Work Experience: {resume_info.get('Work_experience', 'N/A')}\n",
        "Skills: {resume_info.get('Skills', 'N/A')}\n",
        "\n",
        "Job description:\n",
        "Job Title: {job_posting_info.get('Job_title', 'N/A')}\n",
        "Responsibilities: {job_posting_info.get('Responsibilities', 'N/A')}\n",
        "Required Skills: {job_posting_info.get('Required_skills', 'N/A')}\n",
        "Preferred Skills: {job_posting_info.get('Preferred_skills', 'N/A')}\n",
        "Required Experience: {job_posting_info.get('Required_experience', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "behavioral_num_questions = 5\n",
        "\n",
        "# Format the technical questions prompt content separately\n",
        "behavioral_system_content = behavioral_questions_prompt.format(num_questions = behavioral_num_questions)\n",
        "\n",
        "behavioral_user_content = f\"\"\"\n",
        "Candidate's resume:\n",
        "Name: {resume_info.get('Name', 'N/A')}\n",
        "Work Experience: {resume_info.get('Work_experience', 'N/A')}\n",
        "Skills: {resume_info.get('Skills', 'N/A')}\n",
        "\n",
        "Company Profile:\n",
        "Company Name: {company_profile_info.get('Company_Name', 'N/A')}\n",
        "Industry: {company_profile_info.get('Industry', 'N/A')}\n",
        "Mission: {company_profile_info.get('Mission', 'N/A')}\n",
        "Core Values: {company_profile_info.get('Core_values', 'N/A')}\n",
        "Company Culture: {company_profile_info.get('Company_culture', 'N/A')}\n",
        "    \"\"\"\n",
        "\n",
        "# Define the prompt template\n",
        "technical_messages = [\n",
        "    SystemMessage(content=technical_system_content),\n",
        "    HumanMessage(content=technical_user_content)\n",
        "]\n",
        "\n",
        "behavioral_messages = [\n",
        "    SystemMessage(content=behavioral_system_content),\n",
        "    HumanMessage(content=behavioral_user_content)\n",
        "]\n",
        "\n",
        "# Instantiate the chat model\n",
        "generation_model = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# Run the prompt template to generate interview questions\n",
        "technical_response = generation_model.invoke(technical_messages)\n",
        "behavioral_response = generation_model.invoke(behavioral_messages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `technical_questions_response` and `behavioral_questions_response` are AIMessage objects\n",
        "def clean_response(response):\n",
        "    # Access the content attribute directly\n",
        "    content = response.content if hasattr(response, 'content') else response\n",
        "    # Start with an empty list to gather cleaned questions\n",
        "    cleaned_questions = []\n",
        "    # Split the content by lines and process each line\n",
        "    for line in content.split('\\n'):\n",
        "        # Strip unwanted parts like `content=\"` and remove leading/trailing whitespace\n",
        "        cleaned_line = line.strip()\n",
        "        # Append cleaned lines that are not empty\n",
        "        if cleaned_line:\n",
        "            cleaned_questions.append(cleaned_line)\n",
        "    # Join the cleaned questions with line breaks for a more readable format\n",
        "    return '\\n'.join(cleaned_questions)\n",
        "\n",
        "# Clean the technical and behavioral responses\n",
        "cleaned_technical_questions = clean_response(technical_response)\n",
        "cleaned_behavioral_questions = clean_response(behavioral_response)\n",
        "\n",
        "# Combine the responses in a neat format\n",
        "combined_cleaned_output = f\"\"\"\n",
        "Technical Questions:\n",
        "{cleaned_technical_questions}\n",
        "\n",
        "Behavioral Questions:\n",
        "{cleaned_behavioral_questions}\n",
        "\"\"\"\n",
        "\n",
        "# Display the cleaned output\n",
        "print(combined_cleaned_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMpsLYl5tGa0",
        "outputId": "97c53153-a868-4f09-b6c4-5a8f26342047"
      },
      "id": "JMpsLYl5tGa0",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Technical Questions:\n",
            "1. **Transformer-based Model Development:**\n",
            "Given your experience with developing AI assistants using RAG, how would you approach optimizing a transformer-based model for a customer care chatbot to ensure both scalability and efficiency? Please detail any specific techniques or tools you would utilize, especially within a cloud environment like Azure or AWS.\n",
            "2. **Generative AI and LLMs:**\n",
            "You've worked on extracting warranty information using transformer models. Could you describe how you would refine a large language model (LLM) to generate contextually appropriate responses, particularly for complex customer queries? What strategies would you employ for fine-tuning and regularization in PyTorch?\n",
            "3. **AI Integration and API Utilization:**\n",
            "With your background in integrating data from various APIs, how would you go about integrating Azure OpenAI and Assistant API into an AI-driven product, such as a virtual assistant? What challenges might arise, and how would you address them to ensure smooth operation and user experience?\n",
            "4. **Cloud-based AI Model Deployment:**\n",
            "Considering your experience with Google Cloud Platform, how would you handle deploying a scalable AI model on Azure? Discuss any differences you anticipate when transitioning from Google Cloud to Azure, and how you would leverage Azure's features for efficient deployment and management.\n",
            "5. **Research and Innovation:**\n",
            "Having achieved a 15x performance boost in search algorithms during your research, how would you lead a research initiative to explore a new application for transformer and NLP methodologies? Please outline your process for innovation, from hypothesis to implementation, and how you would involve and mentor junior engineers in this process.\n",
            "\n",
            "Behavioral Questions:\n",
            "1. At NRG, we are committed to creating innovative solutions that make our customers' lives easier. Can you share an example of a project where you developed a solution that significantly improved user experience or interaction, and how you measured its success?\n",
            "2. Our mission at NRG is to drive towards a smarter, cleaner future. How have you integrated sustainable or efficient practices in your previous machine learning projects, and how do you envision applying such practices in the energy and home services industry?\n",
            "3. Collaboration and integration across various platforms are crucial to our success at NRG. Can you describe a time when you had to work with multiple APIs and databases, and how you ensured seamless integration to improve project outcomes?\n",
            "4. Given NRG's focus on protecting and intelligently managing homes and businesses, how would your experience with computer vision and AI assist in developing solutions that enhance safety and efficiency for our customers?\n",
            "5. At NRG, we value innovation in our approach to problem-solving. Can you discuss a challenging problem you faced in your past roles as a Machine Learning Engineer and the creative strategies you employed to address it?\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}